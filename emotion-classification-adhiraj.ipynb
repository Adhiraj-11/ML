{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T13:45:11.450955Z","iopub.execute_input":"2022-06-30T13:45:11.451739Z","iopub.status.idle":"2022-06-30T13:45:11.494717Z","shell.execute_reply.started":"2022-06-30T13:45:11.451583Z","shell.execute_reply":"2022-06-30T13:45:11.493955Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional,Dropout\n\nimport re \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:11.496441Z","iopub.execute_input":"2022-06-30T13:45:11.497001Z","iopub.status.idle":"2022-06-30T13:45:24.195911Z","shell.execute_reply.started":"2022-06-30T13:45:11.496961Z","shell.execute_reply":"2022-06-30T13:45:24.195102Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/test.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\ntrain_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/train.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\nvalidation_data = pd.read_csv(\"../input/emotions-dataset-for-nlp/val.txt\", header=None, sep=\";\", names=[\"Comment\",\"Emotion\"], encoding=\"utf-8\")\nprint(\"Train : \", train_data.shape)\nprint(\"Test : \", test_data.shape)\nprint(\"Validation : \", validation_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:24.197429Z","iopub.execute_input":"2022-06-30T13:45:24.198360Z","iopub.status.idle":"2022-06-30T13:45:24.319275Z","shell.execute_reply.started":"2022-06-30T13:45:24.198308Z","shell.execute_reply":"2022-06-30T13:45:24.318265Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:24.321342Z","iopub.execute_input":"2022-06-30T13:45:24.321692Z","iopub.status.idle":"2022-06-30T13:45:24.345639Z","shell.execute_reply.started":"2022-06-30T13:45:24.321655Z","shell.execute_reply":"2022-06-30T13:45:24.344647Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data['length'] = [len(x) for x in train_data['Comment']]\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:24.347263Z","iopub.execute_input":"2022-06-30T13:45:24.347711Z","iopub.status.idle":"2022-06-30T13:45:24.376843Z","shell.execute_reply.started":"2022-06-30T13:45:24.347675Z","shell.execute_reply":"2022-06-30T13:45:24.376093Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_data = {'Train Data': train_data, 'Validation Data': validation_data, 'Test Data': test_data}\nfig, ax = plt.subplots(1,3, figsize=(30,10))\nfor i, df in enumerate(all_data.values()):\n    df2 = df.copy()\n    df2['length'] = [len(x) for x in df2['Comment']]\n    sns.kdeplot(data=df2,x='length',hue='Emotion', ax=ax[i])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:24.378248Z","iopub.execute_input":"2022-06-30T13:45:24.378650Z","iopub.status.idle":"2022-06-30T13:45:25.350972Z","shell.execute_reply.started":"2022-06-30T13:45:24.378609Z","shell.execute_reply":"2022-06-30T13:45:25.350179Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def words_cloud(wordcloud, df):\n    plt.figure(figsize=(10, 10))\n    plt.title(df+' Word Cloud', size = 16)\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:25.352417Z","iopub.execute_input":"2022-06-30T13:45:25.352832Z","iopub.status.idle":"2022-06-30T13:45:25.358299Z","shell.execute_reply.started":"2022-06-30T13:45:25.352790Z","shell.execute_reply":"2022-06-30T13:45:25.357440Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Words Cloud for each emotion","metadata":{}},{"cell_type":"code","source":"emotions_list = train_data['Emotion'].unique()\nemotions_list","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:25.359863Z","iopub.execute_input":"2022-06-30T13:45:25.361049Z","iopub.status.idle":"2022-06-30T13:45:25.377210Z","shell.execute_reply.started":"2022-06-30T13:45:25.360995Z","shell.execute_reply":"2022-06-30T13:45:25.376239Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for emotion in emotions_list:\n    text = ' '.join([sentence for sentence in train_data.loc[train_data['Emotion'] == emotion,'Comment']])\n    wordcloud = WordCloud(width = 600, height = 600).generate(text)\n    words_cloud(wordcloud, emotion)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:25.378769Z","iopub.execute_input":"2022-06-30T13:45:25.379279Z","iopub.status.idle":"2022-06-30T13:45:34.992059Z","shell.execute_reply.started":"2022-06-30T13:45:25.379233Z","shell.execute_reply":"2022-06-30T13:45:34.990977Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"lb = LabelEncoder()\ntrain_data['Emotion'] = lb.fit_transform(train_data['Emotion'])\ntest_data['Emotion'] = lb.fit_transform(test_data['Emotion'])\nvalidation_data['Emotion'] = lb.fit_transform(validation_data['Emotion'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:34.996368Z","iopub.execute_input":"2022-06-30T13:45:34.997380Z","iopub.status.idle":"2022-06-30T13:45:35.012462Z","shell.execute_reply.started":"2022-06-30T13:45:34.997335Z","shell.execute_reply":"2022-06-30T13:45:35.011481Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:35.014022Z","iopub.execute_input":"2022-06-30T13:45:35.014478Z","iopub.status.idle":"2022-06-30T13:45:35.050947Z","shell.execute_reply.started":"2022-06-30T13:45:35.014435Z","shell.execute_reply":"2022-06-30T13:45:35.050227Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### Removing unrelevent stopwords and chars\nText cleaning fuction was inspired by this project - \nhttps://www.kaggle.com/muratkarakurt/emotion-detect-comment-97/notebook","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\nstopwords = set(nltk.corpus.stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:35.052222Z","iopub.execute_input":"2022-06-30T13:45:35.052495Z","iopub.status.idle":"2022-06-30T13:45:55.104567Z","shell.execute_reply.started":"2022-06-30T13:45:35.052462Z","shell.execute_reply":"2022-06-30T13:45:55.103460Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"max_len=train_data['length'].max()\nmax_len","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:55.106245Z","iopub.execute_input":"2022-06-30T13:45:55.107015Z","iopub.status.idle":"2022-06-30T13:45:55.114169Z","shell.execute_reply.started":"2022-06-30T13:45:55.106972Z","shell.execute_reply":"2022-06-30T13:45:55.113389Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# tokenizer = Tokenizer()\n# tokenizer.fit_on_texts(pd.concat(X_train, axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:55.115604Z","iopub.execute_input":"2022-06-30T13:45:55.116541Z","iopub.status.idle":"2022-06-30T13:45:55.125494Z","shell.execute_reply.started":"2022-06-30T13:45:55.116500Z","shell.execute_reply":"2022-06-30T13:45:55.124432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vocabSize = 11000","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:55.127178Z","iopub.execute_input":"2022-06-30T13:45:55.127730Z","iopub.status.idle":"2022-06-30T13:45:55.141003Z","shell.execute_reply.started":"2022-06-30T13:45:55.127689Z","shell.execute_reply":"2022-06-30T13:45:55.139943Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import one_hot\ndef text_cleaning(df, column):\n    \"\"\"Removing unrelevent chars, Stemming and padding\"\"\"\n    stemmer = PorterStemmer()\n    corpus = []\n    \n    for text in df[column]:\n        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n        text = text.lower()\n        text = text.split()\n        text = [stemmer.stem(word) for word in text if word not in stopwords]\n        text = \" \".join(text)\n        corpus.append(text)\n    one_hot_word = [one_hot(input_text=word, n=vocabSize) for word in corpus]\n    pad = pad_sequences(sequences=one_hot_word,maxlen=max_len,padding='pre')\n    print(pad.shape)\n    return pad","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:55.142742Z","iopub.execute_input":"2022-06-30T13:45:55.143714Z","iopub.status.idle":"2022-06-30T13:45:55.155839Z","shell.execute_reply.started":"2022-06-30T13:45:55.143671Z","shell.execute_reply":"2022-06-30T13:45:55.154988Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"x_train = text_cleaning(train_data, \"Comment\")\nx_test = text_cleaning(test_data, \"Comment\")\nx_val = text_cleaning(validation_data, \"Comment\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:45:55.157512Z","iopub.execute_input":"2022-06-30T13:45:55.157912Z","iopub.status.idle":"2022-06-30T13:46:01.707936Z","shell.execute_reply.started":"2022-06-30T13:45:55.157852Z","shell.execute_reply":"2022-06-30T13:46:01.706694Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"y_train = train_data[\"Emotion\"]\ny_test = test_data[\"Emotion\"]\ny_val = validation_data[\"Emotion\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:01.709813Z","iopub.execute_input":"2022-06-30T13:46:01.710433Z","iopub.status.idle":"2022-06-30T13:46:01.716045Z","shell.execute_reply.started":"2022-06-30T13:46:01.710390Z","shell.execute_reply":"2022-06-30T13:46:01.715005Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:01.717874Z","iopub.execute_input":"2022-06-30T13:46:01.718225Z","iopub.status.idle":"2022-06-30T13:46:01.731346Z","shell.execute_reply.started":"2022-06-30T13:46:01.718189Z","shell.execute_reply":"2022-06-30T13:46:01.730499Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Model building\n\nAfter number of tries with higher/lower dim, and adding layers, this model was the most efficient:","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=vocabSize,output_dim=150,input_length=300))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:01.733401Z","iopub.execute_input":"2022-06-30T13:46:01.733778Z","iopub.status.idle":"2022-06-30T13:46:02.264234Z","shell.execute_reply.started":"2022-06-30T13:46:01.733731Z","shell.execute_reply":"2022-06-30T13:46:02.263096Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:02.266047Z","iopub.execute_input":"2022-06-30T13:46:02.266317Z","iopub.status.idle":"2022-06-30T13:46:02.290988Z","shell.execute_reply.started":"2022-06-30T13:46:02.266287Z","shell.execute_reply":"2022-06-30T13:46:02.289720Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:02.292704Z","iopub.execute_input":"2022-06-30T13:46:02.293018Z","iopub.status.idle":"2022-06-30T13:46:02.298984Z","shell.execute_reply.started":"2022-06-30T13:46:02.292979Z","shell.execute_reply":"2022-06-30T13:46:02.297771Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(x_train,y_train,epochs=10,batch_size=64,\n                 validation_data=(x_val,y_val), verbose=1, callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:46:02.300836Z","iopub.execute_input":"2022-06-30T13:46:02.301270Z","iopub.status.idle":"2022-06-30T13:57:12.620380Z","shell.execute_reply.started":"2022-06-30T13:46:02.301215Z","shell.execute_reply":"2022-06-30T13:57:12.619443Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_val,y_val,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:57:12.622688Z","iopub.execute_input":"2022-06-30T13:57:12.623397Z","iopub.status.idle":"2022-06-30T13:57:33.868845Z","shell.execute_reply.started":"2022-06-30T13:57:12.623339Z","shell.execute_reply":"2022-06-30T13:57:33.867824Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test,y_test,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:57:33.870245Z","iopub.execute_input":"2022-06-30T13:57:33.870648Z","iopub.status.idle":"2022-06-30T13:57:42.511539Z","shell.execute_reply.started":"2022-06-30T13:57:33.870615Z","shell.execute_reply":"2022-06-30T13:57:42.510514Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"accuracy = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nloss=hist.history['loss']\nval_loss=hist.history['val_loss']\nepochs=range(len(accuracy))\n\nplt.plot(epochs,accuracy,'b', label='Training accuracy')\nplt.plot(epochs,val_acc,'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs,loss,'b', label='Training loss')\nplt.plot(epochs,val_loss,'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:57:42.512969Z","iopub.execute_input":"2022-06-30T13:57:42.515115Z","iopub.status.idle":"2022-06-30T13:57:42.956982Z","shell.execute_reply.started":"2022-06-30T13:57:42.515066Z","shell.execute_reply":"2022-06-30T13:57:42.955407Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def sentence_cleaning(sentence):\n    \"\"\"Pre-processing sentence for prediction\"\"\"\n    stemmer = PorterStemmer()\n    corpus = []\n    text = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n    text = text.lower()\n    text = text.split()\n    text = [stemmer.stem(word) for word in text if word not in stopwords]\n    text = \" \".join(text)\n    corpus.append(text)\n    one_hot_word = [one_hot(input_text=word, n=vocabSize) for word in corpus]\n    pad = pad_sequences(sequences=one_hot_word,maxlen=max_len,padding='pre')\n    return pad","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:02:17.115449Z","iopub.execute_input":"2022-06-30T15:02:17.115742Z","iopub.status.idle":"2022-06-30T15:02:17.123633Z","shell.execute_reply.started":"2022-06-30T15:02:17.115711Z","shell.execute_reply":"2022-06-30T15:02:17.122693Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sentences = [\n            \"I am really happy today because I met him\"\n            ]\nfor sentence in sentences:\n    print(sentence)\n    sentence = sentence_cleaning(sentence)\n    result = lb.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n    proba =  np.max(model.predict(sentence))\n    print(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T15:02:19.884856Z","iopub.execute_input":"2022-06-30T15:02:19.885486Z","iopub.status.idle":"2022-06-30T15:02:19.912592Z","shell.execute_reply.started":"2022-06-30T15:02:19.885453Z","shell.execute_reply":"2022-06-30T15:02:19.911341Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The model could improve with more playing around with different kind of layers, more complex ones, I assume, but I tried to keep it simple in this project.\n\nOverall not bad results.","metadata":{}}]}